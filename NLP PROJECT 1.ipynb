{"cells":[{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1543,"status":"ok","timestamp":1759199663498,"user":{"displayName":"Abu Tareq Rony","userId":"10716143024340870802"},"user_tz":300},"id":"l29YGD-2r8YW","outputId":"54ab045d-01f9-4504-84c0-ccb078ae56be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1759199663507,"user":{"displayName":"Abu Tareq Rony","userId":"10716143024340870802"},"user_tz":300},"id":"RqwyrkidtBUV"},"outputs":[],"source":["import os\n","project_path = '/content/drive/MyDrive/NLP 11'\n","os.chdir(project_path)"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12155,"status":"ok","timestamp":1759199675670,"user":{"displayName":"Abu Tareq Rony","userId":"10716143024340870802"},"user_tz":300},"id":"oSkZghXZsbWB","outputId":"cf9e6633-671c-4e1b-82bd-d19a2c85c8ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from wordcloud) (11.3.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"]}],"source":["!pip install gensim wordcloud nltk scikit-learn matplotlib\n"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30256,"status":"ok","timestamp":1759199705935,"user":{"displayName":"Abu Tareq Rony","userId":"10716143024340870802"},"user_tz":300},"id":"YkBZ-zVnTdt4","outputId":"06f3b5ba-6591-4cae-f779-80cabfe9d68d"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}],"source":["\"\"\"\n","How to use:\n","1. Save this file as solution.py\n","2. Put QTL_text.json and Trait_dictionary.txt in the same folder\n","3. Run in Spyder (or terminal): python solution.py\n","4. Results will be saved in ./outputs folder\n","\"\"\"\n","\n","import os, json, re, string, logging\n","from collections import Counter\n","\n","import nltk\n","# Minimal, correct downloads\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")\n","nltk.download(\"averaged_perceptron_tagger\")\n","from nltk.corpus import stopwords\n","from nltk import sent_tokenize, word_tokenize\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from gensim.models import Word2Vec\n","from gensim.models.phrases import Phrases, Phraser\n","from wordcloud import WordCloud\n","\n","logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n","INPUT_JSON = \"QTL_text.json\"\n","TRAIT_DICT = \"Trait_dictionary.txt\"\n","OUT_DIR = \"outputs\"\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","EN_STOPWORDS = set(stopwords.words(\"english\"))\n","PUNCT_CHARS = set(string.punctuation)\n","\n","\n","def load_qtl(path):\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","    return [r[\"Abstract\"] for r in data if r.get(\"Category\") == \"1\"]\n","\n","def load_traits(path):\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        return {line.strip().lower() for line in f if line.strip()}\n","\n","def preprocess(texts):\n","    \"\"\"Tokenize, lowercase, remove stopwords & punctuation\"\"\"\n","    tokenized_docs, tokenized_sents = [], []\n","    for abs_text in texts:\n","        doc_tokens = []\n","        for sent in sent_tokenize(abs_text):\n","            tokens = [t.lower() for t in word_tokenize(sent)]\n","            tokens = [t for t in tokens if re.match(r\"[a-z0-9\\-]+\", t)]\n","            tokens = [t for t in tokens if t not in EN_STOPWORDS and t not in PUNCT_CHARS]\n","            if tokens:\n","                tokenized_sents.append(tokens)\n","                doc_tokens.extend(tokens)\n","        if doc_tokens:\n","            tokenized_docs.append(doc_tokens)\n","    return tokenized_docs, tokenized_sents\n","\n","def make_wordcloud(weights, filename):\n","    wc = WordCloud(width=800, height=800, background_color=\"white\")\n","    wc.generate_from_frequencies(weights)\n","    out_path = os.path.join(OUT_DIR, filename)\n","    wc.to_file(out_path)\n","    logging.info(\"Saved %s\", out_path)\n","\n","\n","def task1_wordclouds(tokenized_docs):\n","    term_freqs = Counter([t for doc in tokenized_docs for t in doc])\n","    make_wordcloud(term_freqs, \"qtl_wordcloud_frequency.png\")\n","\n","    joined_texts = [\" \".join(doc) for doc in tokenized_docs]\n","    tfidf_vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n","    tfidf_matrix = tfidf_vectorizer.fit_transform(joined_texts)\n","    term_scores = tfidf_matrix.sum(axis=0).A1\n","    tfidf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), term_scores))\n","    make_wordcloud(tfidf_scores, \"qtl_wordcloud_tfidf.png\")\n","    return tfidf_scores\n","\n","# Task 2: Word2Vec\n","def task2_word2vec(tokenized_sents, tfidf_scores):\n","    w2v_model = Word2Vec(\n","        sentences=tokenized_sents,\n","        vector_size=100,\n","        window=5,\n","        min_count=10,\n","        workers=4\n","    )\n","    top10_terms = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n","    out_path = os.path.join(OUT_DIR, \"qtl_similar_words_raw.txt\")\n","    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n","        for word, _ in top10_terms:\n","            f.write(f\"== {word} ==\\n\")\n","            if word in w2v_model.wv:\n","                for w, s in w2v_model.wv.most_similar(word, topn=20):\n","                    f.write(f\"{w}\\t{s:.4f}\\n\")\n","            else:\n","                f.write(\"(not in vocab)\\n\")\n","            f.write(\"\\n\")\n","    logging.info(\"Saved %s\", out_path)\n","\n","def task3_phrases(tokenized_sents, tokenized_docs, trait_set):\n","\n","    bigram_phr = Phraser(Phrases(tokenized_sents, min_count=5, threshold=5.0))\n","    # Apply bigrams first\n","    sents_bi = [bigram_phr[s] for s in tokenized_sents]\n","    docs_bi = [bigram_phr[d] for d in tokenized_docs]\n","\n","    # Learn trigrams\n","    trigram_phr = Phraser(Phrases(sents_bi, min_count=5, threshold=5.0))\n","    # Apply trigrams\n","    sents_bi_tri = [trigram_phr[s] for s in sents_bi]\n","    docs_bi_tri = [trigram_phr[d] for d in docs_bi]\n","\n","    logging.info(\"Bigrams and trigrams applied to corpus.\")\n","\n","    # Word clouds\n","    term_freqs_phr = Counter([t for doc in docs_bi_tri for t in doc])\n","    make_wordcloud(term_freqs_phr, \"qtl_phrase_wordcloud_frequency.png\")\n","\n","    joined_texts_phr = [\" \".join(doc) for doc in docs_bi_tri]\n","    tfidf_vectorizer_phr = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n","    tfidf_matrix_phr = tfidf_vectorizer_phr.fit_transform(joined_texts_phr)\n","    term_scores_phr = tfidf_matrix_phr.sum(axis=0).A1\n","    tfidf_scores_phr = dict(zip(tfidf_vectorizer_phr.get_feature_names_out(), term_scores_phr))\n","    make_wordcloud(tfidf_scores_phr, \"qtl_phrase_wordcloud_tfidf.png\")\n","\n","    # Word2Vec on phrase-applied sentences\n","    w2v_model_phr = Word2Vec(\n","        sentences=sents_bi_tri,\n","        vector_size=100,\n","        window=5,\n","        min_count=10,\n","        workers=4\n","    )\n","    top10_terms_phr = sorted(tfidf_scores_phr.items(), key=lambda x: x[1], reverse=True)[:10]\n","    out_sim_path = os.path.join(OUT_DIR, \"qtl_similar_words_phrases.txt\")\n","    with open(out_sim_path, \"w\", encoding=\"utf-8\") as f:\n","        for word, _ in top10_terms_phr:\n","            f.write(f\"== {word} ==\\n\")\n","            if word in w2v_model_phr.wv:\n","                for w, s in w2v_model_phr.wv.most_similar(word, topn=20):\n","                    f.write(f\"{w}\\t{s:.4f}\\n\")\n","            else:\n","                f.write(\"(not in vocab)\\n\")\n","            f.write(\"\\n\")\n","\n","    # Dictionary match (normalize underscores -> spaces)\n","    extracted_phrases_set = {t for doc in docs_bi_tri for t in doc if \"_\" in t}\n","    normalized_phrases_set = {p.replace(\"_\", \" \").lower() for p in extracted_phrases_set}\n","    dictionary_matches = [p for p in normalized_phrases_set if p in trait_set]\n","\n","    out_match_path = os.path.join(OUT_DIR, \"qtl_trait_dictionary_match_gensim.txt\")\n","    with open(out_match_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(f\"Total extracted phrases: {len(normalized_phrases_set)}\\n\")\n","        f.write(f\"Matches in trait dictionary: {len(dictionary_matches)}\\n\")\n","        f.write(\"Matched terms:\\n\")\n","        for m in sorted(dictionary_matches):\n","            f.write(m + \"\\n\")\n","    logging.info(\"Saved %s\", out_match_path)\n","\n","\n","# Task 4: NP Chunking + dictionary match\n","def task4_np_chunking(tokenized_sents, trait_set):\n","    grammar = r\"NP: {<DT>?<JJ>{0,2}<NN.*>+}\"\n","    cp = nltk.RegexpParser(grammar)\n","\n","    np_extracted_phrases = []\n","    for sent in tokenized_sents:\n","        tagged = nltk.pos_tag(sent)\n","        tree = cp.parse(tagged)\n","        for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n","            phrase = \" \".join(word for word, pos in subtree.leaves())\n","            np_extracted_phrases.append(phrase.lower())\n","\n","    unique_np_phrases = set(np_extracted_phrases)\n","    np_dictionary_matches = [p for p in unique_np_phrases if p in trait_set]\n","\n","    out_np_path = os.path.join(OUT_DIR, \"qtl_trait_dictionary_match_npchunk.txt\")\n","    with open(out_np_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(f\"Total extracted NP phrases: {len(unique_np_phrases)}\\n\")\n","        f.write(f\"Matches in trait dictionary: {len(np_dictionary_matches)}\\n\")\n","        f.write(\"Matched terms:\\n\")\n","        for m in sorted(np_dictionary_matches):\n","            f.write(m + \"\\n\")\n","\n","    logging.info(\"Saved %s\", out_np_path)\n","\n","\n","# Main\n","def main():\n","    qtl_abstracts = load_qtl(INPUT_JSON)\n","    trait_set = load_traits(TRAIT_DICT)\n","    tokenized_docs, tokenized_sents = preprocess(qtl_abstracts)\n","\n","    tfidf_scores = task1_wordclouds(tokenized_docs)\n","    task2_word2vec(tokenized_sents, tfidf_scores)\n","    task3_phrases(tokenized_sents, tokenized_docs, trait_set)  # now includes trigrams internally\n","    task4_np_chunking(tokenized_sents, trait_set)\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}